{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkH_lUmDCNUV"
      },
      "outputs": [],
      "source": [
        "from pyaltmetric import Altmetric, Citation\n",
        "from pybliometrics.scopus import PlumXMetrics\n",
        "from pyaltmetric import Altmetric\n",
        "from habanero import Crossref\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.agents.agent_toolkits import (\n",
        "    create_vectorstore_agent,\n",
        "    VectorStoreToolkit,\n",
        "    VectorStoreInfo\n",
        ")\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import AgentType\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.utilities import SerpAPIWrapper\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.vectorstores import LanceDB\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain import hub\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import re\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "import math\n",
        "from typing import List, Dict\n",
        "from langchain.schema import Document\n",
        "from langchain.retrievers import BaseRetriever\n",
        "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
        "import math\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from typing import List, Dict\n",
        "from langchain.schema import Document\n",
        "from langchain.retrievers import BaseRetriever\n",
        "from langchain.callbacks.manager import CallbackManagerForRetrieverRun"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the weights for each metric\n",
        "weights = {\n",
        "    'citation': 0.4,\n",
        "    'socialmedia': 0.3,\n",
        "    'capture': 0.2,\n",
        "    'mention': 0.1\n",
        "}\n",
        "\n",
        "def calculate_scores(document_metrics: Dict[str, float]) -> float:\n",
        "    score = sum(weights[key] * math.log(1 + document_metrics.get(key, 0)) for key in weights)\n",
        "    return score\n",
        "\n",
        "class RefRetriever(BaseRetriever):\n",
        "    def __init__(self, db, k: int, weight: float = 0.5):\n",
        "        self.db = db\n",
        "        self.k = k\n",
        "        self.weight = weight\n",
        "        self.metadata_cache = {}  # For caching fetched metadata\n",
        "\n",
        "    async def fetch_metadata(self, session, document_identifier: str) -> Dict[str, float]:\n",
        "        \"\"\"Fetch metadata for a single document asynchronously.\"\"\"\n",
        "        # Check if metadata is already in cache\n",
        "        if document_identifier in self.metadata_cache:\n",
        "            return self.metadata_cache[document_identifier]\n",
        "\n",
        "        # Initialize default metrics\n",
        "        metrics = {'citation': 0, 'socialmedia': 0, 'capture': 0, 'mention': 0}\n",
        "\n",
        "        try:\n",
        "            # Fetch data from Crossref\n",
        "            cr_url = f\"https://api.crossref.org/works?query={document_identifier}\"\n",
        "            async with session.get(cr_url) as resp:\n",
        "                cr_result = await resp.json()\n",
        "                items = cr_result.get('message', {}).get('items', [])\n",
        "                if not items:\n",
        "                    # Cache and return default metrics if no items found\n",
        "                    self.metadata_cache[document_identifier] = metrics\n",
        "                    return metrics\n",
        "                doi = items[0]['DOI']\n",
        "\n",
        "                # Fetch data from PlumXMetrics or Altmetric\n",
        "                # For the sake of example, let's assume we have an async function to fetch PlumXMetrics\n",
        "                # You would need to implement these async fetches as per the API's documentation\n",
        "\n",
        "                # Fetch PlumXMetrics\n",
        "                plumx_url = f\"https://api.elsevier.com/analytics/plumx/doi/{doi}\"\n",
        "                headers = {'Accept': 'application/json'}  # Include necessary headers and API keys\n",
        "                async with session.get(plumx_url, headers=headers) as resp:\n",
        "                    if resp.status == 200:\n",
        "                        plumx_data = await resp.json()\n",
        "                        # Parse plumx_data to extract metrics\n",
        "                        # This is a placeholder; you need to adjust it based on the actual API response\n",
        "                        metrics['citation'] = plumx_data.get('citation', {}).get('total', 0)\n",
        "                        metrics['socialmedia'] = plumx_data.get('socialMedia', {}).get('total', 0)\n",
        "                        metrics['capture'] = plumx_data.get('capture', {}).get('total', 0)\n",
        "                        metrics['mention'] = plumx_data.get('mention', {}).get('total', 0)\n",
        "                    else:\n",
        "                        # If PlumX fails, fallback to Altmetric\n",
        "                        altmetric_url = f\"https://api.altmetric.com/v1/doi/{doi}\"\n",
        "                        async with session.get(altmetric_url) as resp:\n",
        "                            if resp.status == 200:\n",
        "                                altmetric_data = await resp.json()\n",
        "                                # Parse altmetric_data to extract metrics\n",
        "                                metrics['citation'] = altmetric_data.get('cited_by_posts_count', 0)\n",
        "                                metrics['socialmedia'] = altmetric_data.get('cited_by_tweeters_count', 0)\n",
        "                                metrics['capture'] = altmetric_data.get('readers_count', 0)\n",
        "                                metrics['mention'] = altmetric_data.get('cited_by_feeds_count', 0)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching metadata for {document_identifier}: {e}\")\n",
        "            # Keep default metrics in case of any error\n",
        "\n",
        "        # Cache the fetched metrics\n",
        "        self.metadata_cache[document_identifier] = metrics\n",
        "        return metrics\n",
        "\n",
        "    async def fetch_all_metadata(self, document_identifiers: List[str]) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Fetch metadata for all documents concurrently.\"\"\"\n",
        "        metadata_results = {}\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = []\n",
        "            for identifier in document_identifiers:\n",
        "                task = asyncio.ensure_future(self.fetch_metadata(session, identifier))\n",
        "                tasks.append(task)\n",
        "            results = await asyncio.gather(*tasks)\n",
        "            for identifier, metrics in zip(document_identifiers, results):\n",
        "                metadata_results[identifier] = metrics\n",
        "        return metadata_results\n",
        "\n",
        "    async def _aget_relevant_documents(\n",
        "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"Asynchronous method to retrieve relevant documents.\"\"\"\n",
        "        # Fetch 3k documents based on embedding similarity\n",
        "        results_with_scores = self.db.similarity_search_with_score(query, k=self.k * 3)\n",
        "\n",
        "        # Extract unique document identifiers\n",
        "        document_identifiers = []\n",
        "        documents = []\n",
        "        for doc, similarity_score in results_with_scores:\n",
        "            identifier = doc.metadata.get('source', '')\n",
        "            if identifier and identifier not in document_identifiers:\n",
        "                document_identifiers.append(identifier)\n",
        "                documents.append((doc, similarity_score))\n",
        "\n",
        "        # Fetch metadata for all documents\n",
        "        metadata_results = await self.fetch_all_metadata(document_identifiers)\n",
        "\n",
        "        # Combine scores\n",
        "        results_with_combined_scores = []\n",
        "        for (doc, similarity_score) in documents:\n",
        "            d = doc.metadata.get('source', '')\n",
        "            # Retrieve metrics for the document\n",
        "            metrics = metadata_results.get(d, {'citation': 0, 'socialmedia': 0, 'capture': 0, 'mention': 0})\n",
        "            # Calculate the metrics score\n",
        "            metrics_score = calculate_scores(metrics)\n",
        "            # Combine the similarity score and the metrics score\n",
        "            final_score = similarity_score + self.weight * metrics_score\n",
        "            results_with_combined_scores.append((doc, final_score))\n",
        "\n",
        "        # Sort the documents based on the combined score\n",
        "        results_with_combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return the top k documents\n",
        "        top_documents = [doc for doc, _ in results_with_combined_scores[:self.k]]\n",
        "        return top_documents\n",
        "\n",
        "    def _get_relevant_documents(\n",
        "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None\n",
        "    ) -> List[Document]:\n",
        "        \"\"\"Synchronous method that calls the asynchronous method.\"\"\"\n",
        "        return asyncio.run(self._aget_relevant_documents(query, run_manager=run_manager))\n"
      ],
      "metadata": {
        "id": "AZd6DEbFCOKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the embedding function and vector store\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
        "\n",
        "vector = FAISS.load_local(\"faiss_index\", embedding_function, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Instantiate RefRetriever instead of vector.as_retriever\n",
        "k = 5  # Number of documents to retrieve\n",
        "retriever = RefRetriever(db=vector, k=k, weight=0.5)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.01, openai_api_key='YOUR_OPENAI_API_KEY', openai_api_base='https://api.chatgptid.net/v1')\n",
        "\n",
        "retriever_MQ = MultiQueryRetriever.from_llm(\n",
        "    retriever=retriever, llm=llm\n",
        ")\n",
        "\n",
        "retriever_tool = create_retriever_tool(\n",
        "    retriever_MQ,\n",
        "    \"literature_search\",\n",
        "    \"Search for information about input questions. For any questions about battery recycle, you must use this tool!\",\n",
        ")\n",
        "\n",
        "tools = [retriever_tool]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            '''\n",
        "            You are recognized as a leading authority in the field of battery recycling, tasked with the pivotal role of providing expert responses to inquiries. Your responses must embody the highest standards of accuracy, comprehensiveness, and depth of knowledge. Approach each question with a methodical and thoughtful mindset, ensuring your answers are:\n",
        "\n",
        "            Informative: Furnish detailed insights, drawing upon the most relevant facts and figures.\n",
        "            Correct: Ensure factual accuracy in every aspect of your response.\n",
        "            Knowledgeable: Display a profound understanding of the subject matter, including advanced concepts and recent advancements in the field of battery recycling.\n",
        "            Holistic: Offer a well-rounded perspective, considering various facets of the topic at hand.\n",
        "            As you address each question, please adhere to the following guidelines:\n",
        "\n",
        "            Cite Examples: When referencing data or examples from the provided literature, include comprehensive information to contextualize your points effectively. Clearly indicate these instances by stating, \"For example,\" followed by a detailed explanation.\n",
        "            Stay On Topic: Concentrate solely on the query posed. Your reply should be closely aligned with the question, avoiding tangential or unrelated content.\n",
        "            Format Your Answer: Present your response in a structured manner, using either bullet points or numbered lists for clarity and ease of understanding.\n",
        "            Before responding, take a moment to center yourself. Breathe deeply, and proceed with a step-by-step analytical approach, ensuring that your expertise shines through in a manner that is both engaging and enlightening.\n",
        "\n",
        "            ''',\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, reduce_k_below_max_tokens=True)\n",
        "\n",
        "input_question = \"What are the recycling methods for spent lithium-ion batteries?\"\n",
        "\n",
        "breakdown_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", '''\n",
        "        You are an expert in battery recycling, and your task is to answer user's question professionally by first planning what the answer should be composed of.\n",
        "        You need to consider comprehensive aspects in battery recycle to reach the final answers, however your answers must only be in material science, focus on the scientific facts and methodology.\n",
        "        Your job is to search for information in literature review and identify the key aspects that are answers to the input question.\n",
        "        Make sure you are directly answering the subject of the input question(for example: method, definition, components, mechanism), use terminology to describe the aspects\n",
        "        Your answer should be in planning step where you need to set up the framework of the final answers.\n",
        "        ......\n",
        "        '''),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "agent_breakdown = create_openai_functions_agent(llm, tools, breakdown_prompt)\n",
        "\n",
        "agent_breakdown_executor = AgentExecutor(agent=agent_breakdown, tools=tools, verbose=True, reduce_k_below_max_tokens=True)\n",
        "\n",
        "breakdown_a = agent_breakdown_executor.invoke({\"input\": input_question, 'chat_history': []})\n",
        "\n",
        "thoughts = breakdown_a['output']\n",
        "\n",
        "step_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", '''\n",
        "        As an expert in battery recycling, your task is to provide professional answers to user questions. When addressing queries about battery recycling, follow these detailed steps:\n",
        "\n",
        "        Informative: Provide detailed insights, drawing upon the most relevant facts and figures about battery recycling.\n",
        "        Correct: Ensure factual accuracy in every aspect of your response, verifying information against current industry standards and research findings.\n",
        "        Knowledgeable: Demonstrate a profound understanding of the subject matter, including advanced concepts and recent advancements in the field of battery recycling.\n",
        "        Additionally, integrate the following guidelines into your responses:\n",
        "\n",
        "        Stay On Topic: Concentrate solely on the query posed. Ensure that your reply is closely aligned with the question, avoiding tangential or unrelated content.\n",
        "        Literature Review: Before responding, conduct a brief literature search to identify any relevant methods or perspectives that are not commonly discussed. This should include checking the literature search tool to supplement your answer with any missing information.\n",
        "        Analytical Approach: Take a moment to center yourself before responding. Breathe deeply and proceed with a step-by-step analysis, ensuring that your expertise is communicated in a manner that is both engaging and enlightening. Use this approach to critically assess and verify different methods of battery recycling discussed in your literature review.\n",
        "\n",
        "        Describe all the methods and process for the given query, including both traditional techniques and any innovative approaches (especially acronym) in the literature search to make sure you are not neglecting any method.\n",
        "\n",
        "        '''),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "agent_step = create_openai_functions_agent(llm, tools, step_prompt)\n",
        "\n",
        "agent_step_executor = AgentExecutor(agent=agent_step, tools=tools, verbose=True, reduce_k_below_max_tokens=True)\n",
        "\n",
        "thought = agent_step_executor.invoke({\"input\": \"To address the question \" + input_question + \", first list all innovative methods in the literature search which are not mentioned in the previous chat history, and then answer the question\", 'chat_history': [HumanMessage(content=breakdown_a['input']), AIMessage(content=breakdown_a['output'])]})\n",
        "\n",
        "thoughts += thought['output']\n",
        "\n",
        "chat_history = [HumanMessage(content=thought['input']), AIMessage(content=thought['output'])]\n",
        "\n",
        "# Load the second vector store and instantiate RefRetriever\n",
        "vector2 = FAISS.load_local(\"faiss_index2\", embedding_function, allow_dangerous_deserialization=True)\n",
        "\n",
        "k2 = 10\n",
        "retriever2 = RefRetriever(db=vector2, k=k2, weight=0.5)\n",
        "\n",
        "llm2 = ChatOpenAI(model=\"gpt-4\", temperature=0.01, openai_api_key='YOUR_OPENAI_API_KEY')\n",
        "\n",
        "retriever_MQ2 = MultiQueryRetriever.from_llm(\n",
        "    retriever=retriever2, llm=llm2\n",
        ")\n",
        "\n",
        "retriever_tool2 = create_retriever_tool(\n",
        "    retriever_MQ2,\n",
        "    \"literature_search\",\n",
        "    \"Search for information about input questions. For any questions about battery recycle, you must use this tool!\",\n",
        ")\n",
        "\n",
        "tools2 = [retriever_tool2]\n",
        "\n",
        "critique_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", '''\n",
        "\n",
        "        The task requires a detailed critique of an existing explanation concerning technologies for recycling lithium-ion batteries. This critique is intended to serve as guidance for another individual tasked with revising the original answer. The critique must rigorously address specific points:\n",
        "\n",
        "        Fact-Checking:\n",
        "        Scrutinize the original answer for discrepancies or outdated information by cross-verifying each mentioned detail against contemporary research and findings in the lithium-ion battery recycling domain. Highlight inaccuracies and ensure that every fact aligns with the most recent studies and technological advancements.\n",
        "        Supplement:\n",
        "        Dive into the existing literature review to identify and incorporate missing crucial information not covered in the original answer, focus only on material science.\n",
        "        Avoiding Hallucinations:\n",
        "        Identify any baseless assertions, speculative statements, or inaccuracies within the original response. Highlight instances where the presented information lacks direct support from the literature review or recognized sources in the field.\n",
        "        Accuracy and Precision:\n",
        "        Evaluate the technical accuracy and precision of the original answer's descriptions of recycling methods. Identify and correct inaccuracies or over-generalizations, particularly regarding process efficiencies, environmental impacts, and the validation of technological advancements through recent studies.\n",
        "        Literature Alignment:\n",
        "        Assess the degree to which the original answer reflects the insights and conclusions found within the provided literature review. Pinpoint areas where the original response fails to capture the importance of specific studies, technologies, or findings emphasized in the literature.\n",
        "        Instructions for Revision:\n",
        "        The individual responsible for revising the original answer should address all issues identified in this critique. The revised answer must correct any inaccuracies, align with the latest research, and incorporate significant findings from the provided literature accurately.\n",
        "        The revised answer should be factual, precise, and devoid of speculative content, adhering closely to established sources within the field.\n",
        "\n",
        "        The ultimate goal is for the revised answer to be thoroughly informed by this critique, resulting in a comprehensive and accurate response that meets academic standards for inclusion in a literature review on lithium-ion battery recycling.\n",
        "\n",
        "        Provide your answer to the questions as well as your revision to the original question.\n",
        "\n",
        "        '''),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "agent_critique = create_openai_functions_agent(llm2, tools2, critique_prompt)\n",
        "\n",
        "agent_critique_executor = AgentExecutor(agent=agent_critique, tools=tools2, verbose=True, reduce_k_below_max_tokens=True)\n",
        "\n",
        "critique = agent_critique_executor.invoke({\"input\": \"Here's the original question you need to provide insights for: \" + input_question + \" You need to provide your answers and revision to the answer in the chat history.\", 'chat_history': chat_history})\n",
        "\n",
        "chat_history = [HumanMessage(content=thought['input']), AIMessage(content=thought['output']), HumanMessage(content=critique['input']), AIMessage(content=critique['output'])]\n",
        "\n",
        "revise_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", '''\n",
        "\n",
        "        You are an expert in battery recycling, and your task is to answer user's question professionally.\n",
        "        Given the feedback on your initial answer regarding battery recycling, the goal now is to revise your response to make it more informative, accurate, and research-backed.\n",
        "        You are now giving the final answer, make sure the final answer is based on facts and literature.\n",
        "\n",
        "        '''),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "agent_revise = create_openai_functions_agent(llm, tools, revise_prompt)\n",
        "\n",
        "agent_revise_executor = AgentExecutor(agent=agent_revise, tools=tools, verbose=True, reduce_k_below_max_tokens=True)\n",
        "\n",
        "update_answer = agent_revise_executor.invoke({'input': \"Based on the original answer you provided about battery recycling, I've identified several areas that need revision for a more accurate and comprehensive response: \" + critique['output'] + \" Please answer the question based on literature search and revision.\", 'chat_history': chat_history})\n",
        "\n",
        "chat_history = [HumanMessage(content=thought['input']), AIMessage(content=thought['output'])]\n",
        "\n",
        "stop_signal = \"[STOP]\"\n",
        "\n",
        "while True:\n",
        "    critique = agent_critique_executor.invoke({\"input\": \"Here's the original question you need to provide insights for: \" + input_question + \" You need to provide your answers and revision to the answer in the chat history. If you think the answer is correct and accurate, only respond: [STOP]\", 'chat_history': chat_history})\n",
        "\n",
        "    if stop_signal in critique['output']:\n",
        "        break\n",
        "\n",
        "    if len(chat_history) > 2:\n",
        "        chat_history = chat_history[2:]\n",
        "        chat_history.append(HumanMessage(content=critique['input']))\n",
        "        chat_history.append(AIMessage(content=critique['output']))\n",
        "\n",
        "    update_answer = agent_revise_executor.invoke({'input': \"Based on the original answer you provided about battery recycling, I've identified several areas that need revision for a more accurate and comprehensive response: \" + critique['output'] + \" Please answer the question based on literature search and revision.\", 'chat_history': chat_history})\n",
        "\n",
        "    chat_history = [HumanMessage(content=critique['input']), AIMessage(content=critique['output']), HumanMessage(content=update_answer['input']), AIMessage(content=update_answer['output'])]\n"
      ],
      "metadata": {
        "id": "Pg_ZD-WlCVf-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}